{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EnTy1SEajpPV"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"runormas/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py:546: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  class IteratorBase(collections.Iterator, trackable.Trackable,\n",
      "/home/user/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:106: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  class DatasetV2(collections.Iterable, tracking_base.Trackable,\n"
     ]
    }
   ],
   "source": [
    "from modules.data.read import DataReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add answer_sep: <answer>\n",
      "Add start_sep <start>\n"
     ]
    }
   ],
   "source": [
    "reader = DataReader(\n",
    "    path=\"data/public_test/\",\n",
    "    output_dir=\"data/public_test_v6\",\n",
    "    tokenizer_name=\"sberbank-ai/rugpt3xl\",\n",
    "    part=\"test\",\n",
    "    answer_sep=\"<answer>\",\n",
    "    start_sep=\"<start>\",\n",
    "    local_rank=0,\n",
    "    word_size=1,\n",
    "    add_start_sep=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing files from test part on 0...: 100%|██████████| 4370/4370 [00:21<00:00, 203.03it/s]\n",
      "Parsing files from test part on 0...: 100%|██████████| 536/536 [00:03<00:00, 174.26it/s]\n",
      "Making raw files...: 100%|██████████| 4370/4370 [00:00<00:00, 6126.57it/s]\n",
      "Making raw files...: 100%|██████████| 536/536 [00:01<00:00, 361.92it/s]\n"
     ]
    }
   ],
   "source": [
    "reader.prc(is_save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def get_progress(path, reader):\n",
    "    total = defaultdict(int)\n",
    "    done = defaultdict(int)\n",
    "    predicted_anns = defaultdict(dict)\n",
    "    errors = defaultdict(int)\n",
    "    preds = defaultdict(dict)\n",
    "    for data_part in reader.lm_prefixes:\n",
    "        store_dir = os.path.join(path, data_part)\n",
    "        names = list(reader.lm_prefixes[data_part].keys())\n",
    "\n",
    "        for name in names:\n",
    "            fn = os.path.join(store_dir, f\"{name}.norm\")\n",
    "            total[data_part] += len(reader.lm_prefixes[data_part][name])\n",
    "            preds[data_part][name] = []\n",
    "            predicted_anns[data_part][name] = []\n",
    "            if os.path.exists(fn):\n",
    "                with open(fn, 'r', encoding='utf-8') as file:\n",
    "                    predicted_norms = [x.strip() for x in file.read().split(\"\\n\") if x.strip()]\n",
    "\n",
    "                for idx in range(len(reader.lm_prefixes[data_part][name])):\n",
    "                    lm_prefix = reader.lm_prefixes[data_part][name][idx]\n",
    "                    ann = reader.anns[data_part][name][idx]\n",
    "                    if len(predicted_norms) == idx:\n",
    "                        errors[data_part] += len(reader.lm_prefixes[data_part][name]) - idx\n",
    "                        break\n",
    "                    done[data_part] += 1\n",
    "                    pred = predicted_norms[idx]\n",
    "                    text = reader.texts[data_part][name]\n",
    "                    start, stop = list(map(int, ann.split()))\n",
    "                    gen_res = text[start:stop].strip()\n",
    "                    if pred == gen_res:\n",
    "                        predicted_anns[data_part][name].append(idx)\n",
    "                    preds[data_part][name].append(pred)\n",
    "    return total, done, predicted_anns, errors, preds\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix_no_beamsv8_baseline_fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "total, done, predicted_anns, errors, no_beams_preds = get_progress(\"test_pred/fix_no_beams_v21\", reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total, done, predicted_anns, errors, baseline_preds = get_progress(\"test_pred/baseline\", reader)\n",
    "baseline_preds = no_beams_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total, done, predicted_anns, errors, fixed_preds = get_progress(\"test_pred/beams16/\", reader)\n",
    "total, done, predicted_anns, errors, fixed_preds = get_progress(\"test_pred/beams_v2/\", reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'named': 115135, 'generic': 88869})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'named': 115904, 'generic': 89877})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = 0\n",
    "for data_part in predicted_anns:\n",
    "    for name in predicted_anns[data_part]:\n",
    "        tp += len(predicted_anns[data_part][name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68648"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'named': 737, 'generic': 72})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9913646060617841"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(done.values()) / sum(total.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69245.96619674124"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp / sum(done.values()) * sum(total.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scs7xKdhjpPZ"
   },
   "source": [
    "### Test on train and valid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"runormas/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"USE_DEEPSPEED\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.xl_wrapper import RuGPT3XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "Use alternating sparse & dense attention layers\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n"
     ]
    }
   ],
   "source": [
    "model = RuGPT3XL.from_pretrained(\n",
    "    model_name_or_path=\"sberbank-ai/rugpt3xl\",\n",
    "    seq_len=512,\n",
    "    weights_path=\"models/xl/v7/20000/mp_rank_00_model_states.pt\",\n",
    "    deepspeed_config_path=\"runormas/src/deepspeed_config/gpt3_xl_sparse_2048.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tokenizer = reader.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Qw65CVzjpPZ",
    "outputId": "79a31fb8-656d-4923-f208-1589d675d7c7"
   },
   "outputs": [],
   "source": [
    "def filter_results(nr):\n",
    "    return [x[:x.find(\"<|endoftext|>\")][:x.find(\"</s>\")] for x in nr]\n",
    "\n",
    "\n",
    "def generate(model, text, additional_len=32, num_beams=5, do_sample=None):\n",
    "    min_len = min(len(model.tokenizer.encode(text)), 2048 - additional_len)\n",
    "    with torch.no_grad():\n",
    "        return filter_results(model.generate(\n",
    "            text=text,\n",
    "            max_length=min_len + additional_len,\n",
    "            do_sample=do_sample,\n",
    "            num_beams=num_beams,\n",
    "            eos_token_id=model.tokenizer.eos_token_id,\n",
    "            num_return_sequences=1,\n",
    "        ))[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fixed_preds = defaultdict(dict)\n",
    "for data_part in reader.lm_prefixes:\n",
    "\n",
    "    for name in tqdm(reader.lm_prefixes[data_part], total=len(reader.lm_prefixes[data_part])):\n",
    "        fixed_preds[data_part][name] = []\n",
    "        for idx, lm_prefix in tqdm(enumerate(reader.lm_prefixes[data_part][name])):\n",
    "            text = reader.texts[data_part][name]\n",
    "            ann = reader.anns[data_part][name][idx]\n",
    "            start, stop = list(map(int, ann.split()))\n",
    "            ann = text[start:stop].strip()\n",
    "            try:\n",
    "                gen_res = preds[data_part][name][idx]\n",
    "            except KeyboardInterrupt:\n",
    "                raise StopIteration\n",
    "            except:\n",
    "                print(\"Error at\", name)\n",
    "                gen_res = ann\n",
    "            if gen_res == ann:\n",
    "                try:\n",
    "                    gen_res = generate(model, lm_prefix, num_beams=None, do_sample=False)\n",
    "                    gen_res = gen_res.split(reader.answer_sep)\n",
    "                except KeyboardInterrupt:\n",
    "                    raise StopIteration\n",
    "                except:\n",
    "                    print(\"Error at\", name)\n",
    "                    gen_res = [ann]\n",
    "                if len(gen_res) == 1:\n",
    "                    gen_res = text[start:stop].strip()\n",
    "                else:\n",
    "                    gen_res = gen_res[1].strip()\n",
    "            fixed_preds[data_part][name].append(gen_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'постановлять'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_res = generate(model, lm_prefix, num_beams=None, do_sample=False)\n",
    "gen_res = gen_res.split(reader.answer_sep)\n",
    "if len(gen_res) == 1:\n",
    "    gen_res = text[start:stop].strip()\n",
    "else:\n",
    "    gen_res = gen_res[1].strip()\n",
    "gen_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_part in preds:\n",
    "    for name in preds[data_part]:\n",
    "        for idx, pred in enumerate(preds[data_part][name]):\n",
    "            lm_prefix = reader.lm_prefixes[data_part][name][idx]\n",
    "            text = reader.texts[data_part][name]\n",
    "            ann = reader.anns[data_part][name][idx]\n",
    "            start, stop = list(map(int, ann.split()))\n",
    "            ann = text[start:stop].strip()\n",
    "            break\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/lib/python3.7/site-packages/pymorphy2/units/base.py:70: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  args, varargs, kw, default = inspect.getargspec(cls.__init__)\n"
     ]
    }
   ],
   "source": [
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "punctuation = set(punctuation + \"«»\")\n",
    "\n",
    "\n",
    "def fix_verbs(data_part, name, idx, fixed_preds, ann):\n",
    "    if len(fixed_preds[data_part][name][idx].split()) == 1 and data_part == \"generic\":\n",
    "        parsed = morph.parse(fixed_preds[data_part][name][idx])[0]\n",
    "        if parsed.tag.POS in [\"VERB\", \"INFN\"]:\n",
    "            fixed_preds[data_part][name][idx] = parsed.normal_form\n",
    "        \n",
    "def fix_title(data_part, name, idx, fixed_preds, ann):\n",
    "    if ann.istitle():\n",
    "        fixed_preds[data_part][name][idx] = fixed_preds[data_part][name][idx].title()\n",
    "        return int(not fixed_preds[data_part][name][idx].istitle())\n",
    "    return 0\n",
    "\n",
    "\n",
    "def fix_upper_names(data_part, name, idx, fixed_preds, ann, ends=[\"ом\", \"а\", \"у\", \"е\"]):\n",
    "    for x in ends:\n",
    "        if ann.endswith(x) and ann.replace(x, \"\").lower() == fixed_preds[data_part][name][idx]:\n",
    "            if ann.replace(x, \"\").isupper():\n",
    "                # print(fixed_preds[data_part][name][idx], ann)\n",
    "                fixed_preds[data_part][name][idx] = fixed_preds[data_part][name][idx].upper()\n",
    "                return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def fix_e(data_part, name, idx, fixed_preds, ann):\n",
    "    return 0\n",
    "    if \"ё\" in fixed_preds[data_part][name][idx] and \"ё\" not in ann and data_part == \"generic\":\n",
    "        fixed_preds[data_part][name][idx] = fixed_preds[data_part][name][idx].replace(\"ё\", \"е\")\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def fix_special_tokens(data_part, name, idx, fixed_preds, reader):\n",
    "    if sum([x in fixed_preds[data_part][name][idx] for x in [\"<s>\", \"</s>\", reader.answer_sep, reader.start_sep]]):\n",
    "        for x in [\"<s>\", \"</s>\", reader.answer_sep, reader.start_sep]:\n",
    "            fixed_preds[data_part][name][idx] = fixed_preds[data_part][name][idx].replace(f\" {x} \", \" \")\n",
    "            fixed_preds[data_part][name][idx] = fixed_preds[data_part][name][idx].replace(f\"{x} \", \" \")\n",
    "            fixed_preds[data_part][name][idx] = fixed_preds[data_part][name][idx].replace(f\" {x}\", \" \")\n",
    "            fixed_preds[data_part][name][idx] = fixed_preds[data_part][name][idx].replace(f\" {x}\", \" \")\n",
    "            fixed_preds[data_part][name][idx] = fixed_preds[data_part][name][idx].replace(f\"{x}\", \" \").strip()\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def fix_words_count(data_part, name, idx, fixed_preds, ann):\n",
    "    if len(fixed_preds[data_part][name][idx].split()) != len(ann.split()):\n",
    "        # print(ann, fixed_preds[data_part][name][idx], baseline_preds[data_part][name][idx], sep=\" | \")\n",
    "        fixed_preds[data_part][name][idx] = baseline_preds[data_part][name][idx]\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def fix_punct(data_part, name, idx, fixed_preds, ann):\n",
    "    gen_res = fixed_preds[data_part][name][idx]\n",
    "    is_err = False\n",
    "    while len(gen_res) and not gen_res.lower().startswith(ann[0].lower()) and gen_res[0] in punctuation:\n",
    "        gen_res = gen_res[1:]\n",
    "        is_err = True\n",
    "    while len(gen_res) and not gen_res.lower().endswith(ann[-1].lower()) and gen_res[-1] in punctuation:\n",
    "        gen_res = gen_res[:-1]\n",
    "        is_err = True\n",
    "    if len(gen_res):\n",
    "        fixed_preds[data_part][name][idx] = gen_res\n",
    "    if not fixed_preds[data_part][name][idx].lower().startswith(ann[0].lower()):\n",
    "        is_err = True\n",
    "        fixed_preds[data_part][name][idx] = baseline_preds[data_part][name][idx]\n",
    "    return is_err\n",
    "\n",
    "\n",
    "def fix_is_title(data_part, name, idx, fixed_preds, ann):\n",
    "    gen_res = fixed_preds[data_part][name][idx]\n",
    "    try:\n",
    "        if not gen_res[0].istitle() and ann[0].istitle():\n",
    "            fixed_preds[data_part][name][idx] = gen_res[0].title() + gen_res[1:]\n",
    "            return 1\n",
    "    except:\n",
    "        print(data_part, ann, fixed_preds[data_part][name][idx], baseline_preds[data_part][name][idx], sep=\"|\")\n",
    "    return 0\n",
    "\n",
    "\n",
    "def fix_end_sym(data_part, name, idx, fixed_preds, ann):\n",
    "    if ann[-1] in punctuation and fixed_preds[data_part][name][idx][-1] not in punctuation:\n",
    "        fixed_preds[data_part][name][idx] += ann[-1]\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4370/4370 [00:01<00:00, 2823.53it/s]\n",
      "100%|██████████| 536/536 [00:02<00:00, 223.96it/s]\n"
     ]
    }
   ],
   "source": [
    "fixed_preds_anns = 0\n",
    "fixed_predicted_anns = defaultdict(dict)\n",
    "errors = 0\n",
    "is_title_errors = 0\n",
    "compare_baseline = defaultdict(dict)\n",
    "token_errors = 0\n",
    "errors_len = 0\n",
    "intersect_bound = 0.6\n",
    "intersect_errors = 0\n",
    "e_errors = 0\n",
    "is_generate = False\n",
    "upper_errors = 0\n",
    "word_count_err = 0\n",
    "start_sym_err = 0\n",
    "bound_len = 0.5\n",
    "char_len_errors = 0\n",
    "is_title_errors = 0\n",
    "end_sym_errors = 0\n",
    "for data_part in reader.lm_prefixes:\n",
    "\n",
    "    for name in tqdm(reader.lm_prefixes[data_part], total=len(reader.lm_prefixes[data_part])):\n",
    "        fixed_predicted_anns[data_part][name] = []\n",
    "        compare_baseline[data_part][name] = []\n",
    "        for idx, lm_prefix in enumerate(reader.lm_prefixes[data_part][name]):\n",
    "            text = reader.texts[data_part][name]\n",
    "            ann = reader.anns[data_part][name][idx]\n",
    "            start, stop = list(map(int, ann.split()))\n",
    "            ann = text[start:stop].strip()\n",
    "            try:\n",
    "                _ = baseline_preds[data_part][name][idx]\n",
    "            except:\n",
    "                baseline_preds[data_part][name].append(ann)\n",
    "            try:\n",
    "                _ = fixed_preds[data_part][name][idx]\n",
    "            except:\n",
    "                fixed_preds[data_part][name].append(no_beams_preds[data_part][name][idx])\n",
    "            if not len(fixed_preds[data_part][name][idx]):\n",
    "                fixed_preds[data_part][name][idx] = no_beams_preds[data_part][name][idx]\n",
    "            if not len(fixed_preds[data_part][name][idx]):\n",
    "                fixed_preds[data_part][name][idx] = baseline_preds[data_part][name][idx]\n",
    "            if not len(fixed_preds[data_part][name][idx]):\n",
    "                fixed_preds[data_part][name][idx] = ann\n",
    "            token_errors += fix_special_tokens(data_part, name, idx, fixed_preds, reader)\n",
    "            is_title_errors += fix_title(data_part, name, idx, fixed_preds, ann)\n",
    "            upper_errors += fix_upper_names(data_part, name, idx, fixed_preds, ann)\n",
    "            e_errors += fix_e(data_part, name, idx, fixed_preds, ann)\n",
    "            is_title_errors + fix_is_title(data_part, name, idx, fixed_preds, ann)\n",
    "            start_sym_err += fix_punct(data_part, name, idx, fixed_preds, ann)\n",
    "            end_sym_errors + fix_end_sym(data_part, name, idx, fixed_preds, ann)\n",
    "            word_count_err += fix_words_count(data_part, name, idx, fixed_preds, ann)\n",
    "            if len(fixed_preds[data_part][name][idx]) / len(ann) < bound_len:\n",
    "                char_len_errors += 1\n",
    "                fixed_preds[data_part][name][idx] = baseline_preds[data_part][name][idx]\n",
    "            if fixed_preds[data_part][name][idx] == ann:\n",
    "                fixed_preds_anns += 1\n",
    "                fixed_predicted_anns[data_part][name].append(\n",
    "                    {\"ann\": ann, \"gen_res\": fixed_preds[data_part][name][idx],\n",
    "                     \"text\": text, \"lm_prefix\": lm_prefix})\n",
    "            \n",
    "            intersect = len(set(fixed_preds[data_part][name][idx]).intersection(ann))\n",
    "            intersect /= max(1, len(set(fixed_preds[data_part][name][idx])))\n",
    "            if intersect < intersect_bound:\n",
    "                intersect_errors += 1\n",
    "            \n",
    "            if intersect < intersect_bound and is_generate:\n",
    "                # print(ann, fixed_preds[data_part][name][idx], baseline_preds[data_part][name][idx], sep=\"|\")\n",
    "                gen_res = generate(model, lm_prefix, num_beams=None, do_sample=False)\n",
    "                gen_res = gen_res.split(reader.answer_sep)\n",
    "                if len(gen_res) == 1:\n",
    "                    gen_res = text[start:stop].strip()\n",
    "                else:\n",
    "                    gen_res = gen_res[1].strip()\n",
    "                fixed_preds[data_part][name][idx] = gen_res\n",
    "\n",
    "                fix_special_tokens(data_part, name, idx, fixed_preds, reader)\n",
    "                fix_title(data_part, name, idx, fixed_preds, ann)\n",
    "                fix_upper_names(data_part, name, idx, fixed_preds, ann)\n",
    "                fix_e(data_part, name, idx, fixed_preds, ann)\n",
    "                word_count_err += fix_words_count(data_part, name, idx, fixed_preds, ann)\n",
    "                is_title_errors + fix_is_title(data_part, name, idx, fixed_preds, ann)\n",
    "                start_sym_err += fix_punct(data_part, name, idx, fixed_preds, ann)\n",
    "                end_sym_errors + fix_end_sym(data_part, name, idx, fixed_preds, ann)\n",
    "                intersect = len(set(fixed_preds[data_part][name][idx]).intersection(ann))\n",
    "                intersect /= max(1, len(set(fixed_preds[data_part][name][idx])))\n",
    "                if intersect < 0.6:\n",
    "                    print(data_part, ann, fixed_preds[data_part][name][idx], baseline_preds[data_part][name][idx], sep=\"|\")\n",
    "            elif intersect < intersect_bound:\n",
    "                fixed_preds[data_part][name][idx] = baseline_preds[data_part][name][idx]\n",
    "            fix_verbs(data_part, name, idx, fixed_preds, ann)\n",
    "            if fixed_preds[data_part][name][idx] != baseline_preds[data_part][name][idx]:\n",
    "                compare_baseline[data_part][name].append(\n",
    "                    {\"ann\": ann,\n",
    "                     \"gen_res\": fixed_preds[data_part][name][idx],\n",
    "                     \"text\": text,\n",
    "                     \"lm_prefix\": lm_prefix,\n",
    "                     \"baseline\": baseline_preds[data_part][name][idx]\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_title_errors': 0,\n",
       " 'token_errors': 0,\n",
       " 'intersect_errors': 48,\n",
       " 'e_errors': 0,\n",
       " 'upper_errors': 0,\n",
       " 'word_count_err': 0,\n",
       " 'char_len_errors': 0,\n",
       " 'start_sym_err': 10}"
      ]
     },
     "execution_count": 628,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"is_title_errors\": is_title_errors,\n",
    "    \"token_errors\": token_errors,\n",
    "    \"intersect_errors\": intersect_errors,\n",
    "    \"e_errors\": e_errors,\n",
    "    \"upper_errors\": upper_errors,\n",
    "    \"word_count_err\": word_count_err,\n",
    "    \"char_len_errors\": char_len_errors,\n",
    "    \"start_sym_err\": start_sym_err\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "# errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VERB'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph.parse(\"горит\")[0].tag.POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\"ADJF\", \"NOUN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_counts = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_baseline_count = 0\n",
    "for data_part in [\"generic\"]:\n",
    "    for name in list(reader.lm_prefixes[data_part]):\n",
    "        for idx, lm_prefix in enumerate(reader.lm_prefixes[data_part][name]):\n",
    "            text = reader.texts[data_part][name]\n",
    "            ann = reader.anns[data_part][name][idx]\n",
    "            start, stop = list(map(int, ann.split()))\n",
    "            ann = text[start:stop].strip()\n",
    "            gen_res = fixed_preds[data_part][name][idx]\n",
    "            if len(gen_res.split()) == 1:\n",
    "                parsed = morph.parse(gen_res)[0]\n",
    "                if parsed.tag.POS in [\"ADJF\", \"NOUN\", \"VERB\"]:\n",
    "                    single_counts += 1\n",
    "                    fixed_preds[data_part][name][idx] = parsed.normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93029"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = 0\n",
    "for data_part in [\"named\"]:\n",
    "    for name in list(reader.lm_prefixes[data_part]):\n",
    "        for idx, lm_prefix in enumerate(reader.lm_prefixes[data_part][name]):\n",
    "            text = reader.texts[data_part][name]\n",
    "            ann = reader.anns[data_part][name][idx]\n",
    "            start, stop = list(map(int, ann.split()))\n",
    "            ann = text[start:stop].strip()\n",
    "            gen_res = fixed_preds[data_part][name][idx]\n",
    "            if sum([x.istitle() for x in gen_res.split()]) != sum([x.istitle() for x in ann.split()]):\n",
    "                errors += 1\n",
    "            # print(data_part, ann, fixed_preds[data_part][name][idx], baseline_preds[data_part][name][idx], sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0007678768636112644"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors / total[\"named\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9659152205500022"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_baseline_count/sum(total.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"test_pred/beams_v2_single_word_fix.2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predict on 0: 100%|██████████| 4370/4370 [00:25<00:00, 168.92it/s]\n",
      "Predict on 0: 100%|██████████| 536/536 [00:01<00:00, 347.99it/s]\n"
     ]
    }
   ],
   "source": [
    "for data_part in fixed_preds:\n",
    "    store_dir = os.path.join(path, data_part)\n",
    "    if not os.path.exists(store_dir):\n",
    "        os.makedirs(store_dir, exist_ok=True)\n",
    "    names = list(fixed_preds[data_part].keys())\n",
    "\n",
    "    for name in tqdm(names, total=len(names), leave=True, desc=f\"Predict on {0}\"):\n",
    "        with open(os.path.join(store_dir, f\"{name}.norm\"), 'w', encoding='utf-8') as file:\n",
    "            for pred in fixed_preds[data_part][name]:\n",
    "                file.write(f\"{pred}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ruGPT3XL_generation",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
