{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 26/536 [19:54<6:56:22, 48.99s/it] "
     ]
    }
   ],
   "source": [
    "from os import listdir, mkdir\n",
    "from os.path import join, exists\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import shutil\n",
    "\n",
    "import os\n",
    "\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    Doc,\n",
    ")\n",
    "\n",
    "from natasha.doc import DocSpan\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "\n",
    "\n",
    "if os.path.exists(f\"baseline\"):\n",
    "    shutil.rmtree(f\"baseline\")\n",
    "\n",
    "mkdir(f\"baseline\")\n",
    "mkdir(f\"baseline/generic\")\n",
    "mkdir(f\"baseline/named\")\n",
    "\n",
    "for part in [\"generic\", \"named\"]:\n",
    "    texts = {}\n",
    "    anns = {}\n",
    "\n",
    "    files = os.listdir(f\"data/public_test/{part}\")\n",
    "    \n",
    "    for file in files:\n",
    "        name = file[:-4]\n",
    "\n",
    "        if file[-3:] == \"txt\":\n",
    "            text = open(f\"data/public_test/{part}/{file}\", encoding='utf-8').read()\n",
    "\n",
    "            texts[name] = text\n",
    "        elif file[-3:] == \"ann\":\n",
    "            ann = open(f\"data/public_test/{part}/{file}\", encoding='utf-8').read().strip().split('\\n')\n",
    "\n",
    "            anns[name] = ann\n",
    "    \n",
    "    for name in tqdm(texts):\n",
    "        text = texts[name]\n",
    "        \n",
    "        ann = anns[name]\n",
    "        \n",
    "        f = open(f\"baseline/{part}/{name}.norm\", 'w', encoding='utf-8')\n",
    "        \n",
    "        for line in ann:\n",
    "            spans = list(map(int, line.strip().split()))\n",
    "            entry = ''\n",
    "            while spans:\n",
    "                start, stop = spans[0], spans[1]\n",
    "                entry += text[start:stop] + \" \"\n",
    "                \n",
    "                spans = spans[2:]\n",
    "            \n",
    "            entry = entry.strip()\n",
    "            \n",
    "            doc = Doc(entry)\n",
    "\n",
    "            doc.segment(segmenter)\n",
    "\n",
    "            doc.tag_morph(morph_tagger)\n",
    "            doc.parse_syntax(syntax_parser)\n",
    "            doc.tag_ner(ner_tagger)\n",
    "\n",
    "            found = False\n",
    "            for s in doc.spans:\n",
    "                if s.text == entry:\n",
    "                    span = s\n",
    "                    found = True\n",
    "                    break\n",
    "\n",
    "            if not found:\n",
    "                span = DocSpan(\n",
    "                    start=0\n",
    "                    , stop=len(entry)\n",
    "                    , type='ORG'\n",
    "                    , text=entry\n",
    "                    , tokens=[token for token in doc.tokens]\n",
    "                )\n",
    "            \n",
    "            span.normalize(morph_vocab)\n",
    "\n",
    "            f.write(f\"{span.normal}\\n\")\n",
    "            \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry = \"консолидированный\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Doc(entry)\n",
    "\n",
    "doc.segment(segmenter)\n",
    "\n",
    "doc.tag_morph(morph_tagger)\n",
    "doc.parse_syntax(syntax_parser)\n",
    "doc.tag_ner(ner_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "found = False\n",
    "for s in doc.spans:\n",
    "    if s.text == entry:\n",
    "        span = s\n",
    "        found = True\n",
    "        break\n",
    "\n",
    "if not found:\n",
    "    span = DocSpan(\n",
    "        start=0\n",
    "        , stop=len(entry)\n",
    "        , type='ORG'\n",
    "        , text=entry\n",
    "        , tokens=[token for token in doc.tokens]\n",
    "    )\n",
    "\n",
    "span.normalize(morph_vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
