{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EnTy1SEajpPV"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"runormas/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py:546: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  class IteratorBase(collections.Iterator, trackable.Trackable,\n",
      "/home/user/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:106: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  class DatasetV2(collections.Iterable, tracking_base.Trackable,\n"
     ]
    }
   ],
   "source": [
    "from modules.data.read import DataReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add answer_sep: <answer>\n",
      "Add start_sep <start>\n",
      "Add start_sep <end>\n"
     ]
    }
   ],
   "source": [
    "reader = DataReader(\n",
    "    path=\"data/public_test/\",\n",
    "    output_dir=\"data/public_test_v13\",\n",
    "    tokenizer_name=\"sberbank-ai/rugpt3xl\",\n",
    "    part=\"test\",\n",
    "    local_rank=0,\n",
    "    word_size=1,\n",
    "    window_size=40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing files from test part on 0...: 100%|██████████| 4906/4906 [00:11<00:00, 437.74it/s]\n",
      "Making raw files...: 100%|██████████| 4370/4370 [00:03<00:00, 1338.33it/s]\n",
      "Making raw files...: 100%|██████████| 536/536 [00:09<00:00, 57.14it/s]\n"
     ]
    }
   ],
   "source": [
    "reader.prc(is_save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.utils import get_all_files_from_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_all_files_from_dir(\"test_pred/v14_90k_fixed/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4906"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_preds = defaultdict(dict)\n",
    "fixed_preds = defaultdict(dict)\n",
    "errors = []\n",
    "for fn in files:\n",
    "    n_key = os.path.basename(fn)\n",
    "    part = \"named\" if \"named\" in fn else \"generic\"\n",
    "    key = n_key[:-5]\n",
    "    baseline_preds[part][key] = []\n",
    "    fixed_preds[part][key] = []\n",
    "    with open(fn) as file:\n",
    "        baseline_preds[part][key] = file.read().strip().split(\"\\n\")\n",
    "    fn = os.path.join(\"test_pred/v14_130k_finetune/\", part, n_key)\n",
    "    if os.path.exists(fn):\n",
    "        try:\n",
    "            with open(fn) as file:\n",
    "                fixed_preds[part][key] = file.read().strip().split(\"\\n\")\n",
    "        except:\n",
    "            print(\"error at\", fn)\n",
    "    else:\n",
    "        fixed_preds[part][key] = baseline_preds[part][key]\n",
    "        errors.append(fn)\n",
    "    for idx in range(len(fixed_preds[part][key])):\n",
    "        if len(fixed_preds[part][key]) == idx:\n",
    "            fixed_preds[part][key].append(baseline_preds[part][key][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here 10 files was not predicted because trancated while distributed generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_beams_preds = baseline_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "punctuation = set(punctuation + \"«»\")\n",
    "\n",
    "\n",
    "def fix_verbs(data_part, name, idx, fixed_preds, ann):\n",
    "    if len(fixed_preds[data_part][name][idx].split()) == 1 and data_part == \"generic\":\n",
    "        parsed = morph.parse(fixed_preds[data_part][name][idx])[0]\n",
    "        if parsed.tag.POS in [\"VERB\", \"INFN\"]:\n",
    "            fixed_preds[data_part][name][idx] = parsed.normal_form\n",
    "        \n",
    "def fix_title(data_part, name, idx, fixed_preds, ann):\n",
    "    if ann.istitle():\n",
    "        fixed_preds[data_part][name][idx] = fixed_preds[data_part][name][idx].title()\n",
    "        return int(not fixed_preds[data_part][name][idx].istitle())\n",
    "    return 0\n",
    "\n",
    "\n",
    "def fix_upper_names(data_part, name, idx, fixed_preds, ann, ends=[\"ом\", \"а\", \"у\", \"е\"]):\n",
    "    for x in ends:\n",
    "        if ann.endswith(x) and ann.replace(x, \"\").lower() == fixed_preds[data_part][name][idx]:\n",
    "            if ann.replace(x, \"\").isupper():\n",
    "                # print(fixed_preds[data_part][name][idx], ann)\n",
    "                fixed_preds[data_part][name][idx] = fixed_preds[data_part][name][idx].upper()\n",
    "                return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def fix_e(data_part, name, idx, fixed_preds, ann):\n",
    "    return 0\n",
    "    if \"ё\" in fixed_preds[data_part][name][idx] and \"ё\" not in ann and data_part == \"generic\":\n",
    "        fixed_preds[data_part][name][idx] = fixed_preds[data_part][name][idx].replace(\"ё\", \"е\")\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def fix_special_tokens(data_part, name, idx, fixed_preds, reader):\n",
    "    if sum([x in fixed_preds[data_part][name][idx] for x in [\"<s>\", \"</s>\", reader.answer_sep, reader.start_sep]]):\n",
    "        for x in [\"<s>\", \"</s>\", reader.answer_sep, reader.start_sep]:\n",
    "            fixed_preds[data_part][name][idx] = fixed_preds[data_part][name][idx].replace(f\" {x} \", \" \")\n",
    "            fixed_preds[data_part][name][idx] = fixed_preds[data_part][name][idx].replace(f\"{x} \", \" \")\n",
    "            fixed_preds[data_part][name][idx] = fixed_preds[data_part][name][idx].replace(f\" {x}\", \" \")\n",
    "            fixed_preds[data_part][name][idx] = fixed_preds[data_part][name][idx].replace(f\" {x}\", \" \")\n",
    "            fixed_preds[data_part][name][idx] = fixed_preds[data_part][name][idx].replace(f\"{x}\", \" \").strip()\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def fix_words_count(data_part, name, idx, fixed_preds, ann):\n",
    "    if len(fixed_preds[data_part][name][idx].split()) != len(ann.split()):\n",
    "        # print(ann, fixed_preds[data_part][name][idx], baseline_preds[data_part][name][idx], sep=\" | \")\n",
    "        fixed_preds[data_part][name][idx] = baseline_preds[data_part][name][idx]\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def fix_punct(data_part, name, idx, fixed_preds, ann):\n",
    "    gen_res = fixed_preds[data_part][name][idx]\n",
    "    is_err = False\n",
    "    while len(gen_res) and not gen_res.lower().startswith(ann[0].lower()) and gen_res[0] in punctuation:\n",
    "        gen_res = gen_res[1:]\n",
    "        is_err = True\n",
    "    while len(gen_res) and not gen_res.lower().endswith(ann[-1].lower()) and gen_res[-1] in punctuation:\n",
    "        gen_res = gen_res[:-1]\n",
    "        is_err = True\n",
    "    if len(gen_res):\n",
    "        fixed_preds[data_part][name][idx] = gen_res\n",
    "    if not fixed_preds[data_part][name][idx].lower().startswith(ann[0].lower()):\n",
    "        is_err = True\n",
    "        fixed_preds[data_part][name][idx] = baseline_preds[data_part][name][idx]\n",
    "    return is_err\n",
    "\n",
    "\n",
    "def fix_is_title(data_part, name, idx, fixed_preds, ann):\n",
    "    gen_res = fixed_preds[data_part][name][idx]\n",
    "    try:\n",
    "        if not gen_res[0].istitle() and ann[0].istitle():\n",
    "            fixed_preds[data_part][name][idx] = gen_res[0].title() + gen_res[1:]\n",
    "            return 1\n",
    "    except:\n",
    "        print(data_part, ann, fixed_preds[data_part][name][idx], baseline_preds[data_part][name][idx], sep=\"|\")\n",
    "    return 0\n",
    "\n",
    "\n",
    "def fix_end_sym(data_part, name, idx, fixed_preds, ann):\n",
    "    if ann[-1] in punctuation and fixed_preds[data_part][name][idx][-1] not in punctuation:\n",
    "        fixed_preds[data_part][name][idx] += ann[-1]\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4370/4370 [00:01<00:00, 2916.24it/s]\n",
      "100%|██████████| 536/536 [00:02<00:00, 235.16it/s]\n"
     ]
    }
   ],
   "source": [
    "fixed_preds_anns = 0\n",
    "fixed_predicted_anns = defaultdict(dict)\n",
    "errors = 0\n",
    "is_title_errors = 0\n",
    "compare_baseline = defaultdict(dict)\n",
    "token_errors = 0\n",
    "errors_len = 0\n",
    "intersect_bound = 0.6\n",
    "intersect_errors = 0\n",
    "e_errors = 0\n",
    "is_generate = False\n",
    "upper_errors = 0\n",
    "word_count_err = 0\n",
    "start_sym_err = 0\n",
    "bound_len = 0.5\n",
    "char_len_errors = 0\n",
    "is_title_errors = 0\n",
    "end_sym_errors = 0\n",
    "for data_part in reader.lm_prefixes:\n",
    "\n",
    "    for name in tqdm(reader.lm_prefixes[data_part], total=len(reader.lm_prefixes[data_part])):\n",
    "        fixed_predicted_anns[data_part][name] = []\n",
    "        compare_baseline[data_part][name] = []\n",
    "        for idx, lm_prefix in enumerate(reader.lm_prefixes[data_part][name]):\n",
    "            text = reader.texts[data_part][name]\n",
    "            ann = reader.anns[data_part][name][idx]\n",
    "            start, stop = list(map(int, ann.split()))\n",
    "            ann = text[start:stop].strip()\n",
    "            try:\n",
    "                _ = baseline_preds[data_part][name][idx]\n",
    "            except:\n",
    "                baseline_preds[data_part][name].append(ann)\n",
    "            try:\n",
    "                _ = fixed_preds[data_part][name][idx]\n",
    "            except:\n",
    "                fixed_preds[data_part][name].append(no_beams_preds[data_part][name][idx])\n",
    "            if not len(fixed_preds[data_part][name][idx]):\n",
    "                fixed_preds[data_part][name][idx] = no_beams_preds[data_part][name][idx]\n",
    "            if not len(fixed_preds[data_part][name][idx]):\n",
    "                fixed_preds[data_part][name][idx] = baseline_preds[data_part][name][idx]\n",
    "            if not len(fixed_preds[data_part][name][idx]):\n",
    "                fixed_preds[data_part][name][idx] = ann\n",
    "            token_errors += fix_special_tokens(data_part, name, idx, fixed_preds, reader)\n",
    "            is_title_errors += fix_title(data_part, name, idx, fixed_preds, ann)\n",
    "            upper_errors += fix_upper_names(data_part, name, idx, fixed_preds, ann)\n",
    "            e_errors += fix_e(data_part, name, idx, fixed_preds, ann)\n",
    "            is_title_errors + fix_is_title(data_part, name, idx, fixed_preds, ann)\n",
    "            start_sym_err += fix_punct(data_part, name, idx, fixed_preds, ann)\n",
    "            end_sym_errors + fix_end_sym(data_part, name, idx, fixed_preds, ann)\n",
    "            word_count_err += fix_words_count(data_part, name, idx, fixed_preds, ann)\n",
    "            if len(fixed_preds[data_part][name][idx]) / len(ann) < bound_len:\n",
    "                char_len_errors += 1\n",
    "                fixed_preds[data_part][name][idx] = baseline_preds[data_part][name][idx]\n",
    "            if fixed_preds[data_part][name][idx] == ann:\n",
    "                fixed_preds_anns += 1\n",
    "                fixed_predicted_anns[data_part][name].append(\n",
    "                    {\"ann\": ann, \"gen_res\": fixed_preds[data_part][name][idx],\n",
    "                     \"text\": text, \"lm_prefix\": lm_prefix})\n",
    "            \n",
    "            intersect = len(set(fixed_preds[data_part][name][idx]).intersection(ann))\n",
    "            intersect /= max(1, len(set(fixed_preds[data_part][name][idx])))\n",
    "            if intersect < intersect_bound:\n",
    "                intersect_errors += 1\n",
    "            \n",
    "            if intersect < intersect_bound and is_generate:\n",
    "                # print(ann, fixed_preds[data_part][name][idx], baseline_preds[data_part][name][idx], sep=\"|\")\n",
    "                gen_res = generate(model, lm_prefix, num_beams=None, do_sample=False)\n",
    "                gen_res = gen_res.split(reader.answer_sep)\n",
    "                if len(gen_res) == 1:\n",
    "                    gen_res = text[start:stop].strip()\n",
    "                else:\n",
    "                    gen_res = gen_res[1].strip()\n",
    "                if len(gen_res.split()) == 1 and \"област\" in gen_res:\n",
    "                    gen_res = \"область\"\n",
    "                    \n",
    "                fixed_preds[data_part][name][idx] = gen_res\n",
    "\n",
    "                fix_special_tokens(data_part, name, idx, fixed_preds, reader)\n",
    "                fix_title(data_part, name, idx, fixed_preds, ann)\n",
    "                fix_upper_names(data_part, name, idx, fixed_preds, ann)\n",
    "                fix_e(data_part, name, idx, fixed_preds, ann)\n",
    "                word_count_err += fix_words_count(data_part, name, idx, fixed_preds, ann)\n",
    "                is_title_errors + fix_is_title(data_part, name, idx, fixed_preds, ann)\n",
    "                start_sym_err += fix_punct(data_part, name, idx, fixed_preds, ann)\n",
    "                end_sym_errors + fix_end_sym(data_part, name, idx, fixed_preds, ann)\n",
    "                intersect = len(set(fixed_preds[data_part][name][idx]).intersection(ann))\n",
    "                intersect /= max(1, len(set(fixed_preds[data_part][name][idx])))\n",
    "                if intersect < 0.6:\n",
    "                    print(data_part, ann, fixed_preds[data_part][name][idx], baseline_preds[data_part][name][idx], sep=\"|\")\n",
    "            elif intersect < intersect_bound:\n",
    "                fixed_preds[data_part][name][idx] = baseline_preds[data_part][name][idx]\n",
    "            fix_verbs(data_part, name, idx, fixed_preds, ann)\n",
    "            if fixed_preds[data_part][name][idx] != baseline_preds[data_part][name][idx]:\n",
    "                compare_baseline[data_part][name].append(\n",
    "                    {\"ann\": ann,\n",
    "                     \"gen_res\": fixed_preds[data_part][name][idx],\n",
    "                     \"text\": text,\n",
    "                     \"lm_prefix\": lm_prefix,\n",
    "                     \"baseline\": baseline_preds[data_part][name][idx]\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_title_errors': 0,\n",
       " 'token_errors': 0,\n",
       " 'errors_len': 0,\n",
       " 'intersect_errors': 35,\n",
       " 'e_errors': 0,\n",
       " 'upper_errors': 0,\n",
       " 'word_count_err': 0,\n",
       " 'char_len_errors': 0,\n",
       " 'start_sym_err': 6,\n",
       " 'end_sym_errors': 0}"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"is_title_errors\": is_title_errors,\n",
    "    \"token_errors\": token_errors,\n",
    "    \"errors_len\": errors_len,\n",
    "    \"intersect_errors\": intersect_errors,\n",
    "    \"e_errors\": e_errors,\n",
    "    \"upper_errors\": upper_errors,\n",
    "    \"word_count_err\": word_count_err,\n",
    "    \"char_len_errors\": char_len_errors,\n",
    "    \"start_sym_err\": start_sym_err,\n",
    "    \"is_title_errors\": is_title_errors,\n",
    "    \"end_sym_errors\": end_sym_errors\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fix ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = 0\n",
    "for data_part in [\"named\"]:\n",
    "    for name in list(reader.lm_prefixes[data_part]):\n",
    "        for idx, lm_prefix in enumerate(reader.lm_prefixes[data_part][name]):\n",
    "            text = reader.texts[data_part][name]\n",
    "            ann = reader.anns[data_part][name][idx]\n",
    "            start, stop = list(map(int, ann.split()))\n",
    "            ann = text[start:stop].strip()\n",
    "            gen_res = fixed_preds[data_part][name][idx]\n",
    "            # if sum([x.istitle() for x in gen_res.split()]) != sum([x.istitle() for x in ann.split()]):\n",
    "            #     errors += 1\n",
    "            if ann.count(\" \") != fixed_preds[data_part][name][idx].count(\" \"):\n",
    "                #print(data_part, ann, fixed_preds[data_part][name][idx], baseline_preds[data_part][name][idx], sep=\"|\")\n",
    "                #continue\n",
    "                words = ann.split()\n",
    "                text_ann = ann\n",
    "                ws = []\n",
    "                gen_words = gen_res.split()\n",
    "                new_gen_res = gen_words[0]\n",
    "                assert len(words) == len(gen_words)\n",
    "                for idx_, (word, gen_word) in enumerate(zip(words[:-1], gen_words[1:])):\n",
    "                    pos = text_ann.find(word)\n",
    "                    while pos < len(text_ann) and not text_ann[pos:].startswith(words[idx_ + 1]):\n",
    "                        pos += 1\n",
    "                    new_gen_res += text_ann[len(word):pos] + gen_word\n",
    "                    text_ann = text_ann[pos:]\n",
    "                fixed_preds[data_part][name][idx] = new_gen_res\n",
    "                errors += 1\n",
    "                # print(ann.count(\" \"), ann.split())\n",
    "                # print(data_part, ann, fixed_preds[data_part][name][idx], baseline_preds[data_part][name][idx], sep=\"|\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"test_pred/v14_130k_finetune_fixed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predict on 0: 100%|██████████| 4370/4370 [00:10<00:00, 416.63it/s]\n",
      "Predict on 0: 100%|██████████| 536/536 [00:01<00:00, 387.61it/s]\n"
     ]
    }
   ],
   "source": [
    "for data_part in fixed_preds:\n",
    "    store_dir = os.path.join(path, data_part)\n",
    "    if not os.path.exists(store_dir):\n",
    "        os.makedirs(store_dir, exist_ok=True)\n",
    "    names = list(fixed_preds[data_part].keys())\n",
    "\n",
    "    for name in tqdm(names, total=len(names), leave=True, desc=f\"Predict on {0}\"):\n",
    "        with open(os.path.join(store_dir, f\"{name}.norm\"), 'w', encoding='utf-8') as file:\n",
    "            for pred in fixed_preds[data_part][name]:\n",
    "                file.write(f\"{pred}\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ruGPT3XL_generation",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
